# python

### 

# Python 面向对象

1。类有一个名为 __init__() 的特殊方法（构造方法），该方法在类实例化时会自动调用

2。类的方法：类的方法与普通的函数只有一个特别的区别——它们必须有一个额外的第一个参数名称, 按照惯例它的名称是 self。

3。在类的内部，使用 def 关键字来定义一个方法，与一般函数定义不同，类方法必须包含参数 self, 且为第一个参数，self 代表的是类的实例。

4。类的私有属性：__private_attrs：两个下划线开头，声明该属性为私有，不能在类的外部被使用或直接访问。在类内部的方法中使用时 self.__private_attrs。

5。类的私有方法：__private_method：两个下划线开头，声明该方法为私有方法，不能在类的外部调用。在类的内部调用 self.__private_methods

6。单下划线、双下划线、头尾双下划线说明：
* __foo__: 定义的是特殊方法，一般是系统定义名字 ，类似 __init__() 之类的。

* _foo: 以单下划线开头的表示的是 protected 类型的变量，即保护类型只能允许其本身与子类进行访问，不能用于 from module import *

* __foo: 双下划线的表示的是私有类型(private)的变量, 只能是允许这个类本身进行访问了。

---

为什么说pytorch是动态的？

Tensorflow 就是最典型的静态计算模块, 大部分时候, 用 Tensorflow 是先搭建好这样一个计算系统, 一旦搭建好了, 就不能改动了 (也有例外, 比如`dynamic_rnn()`, 但是总体来说他还是运用了一个静态思维), 所有的计算都会在这种图中流动。不动结构当然可以提高效率.

需要动态的最典型的例子就是 RNN, 有时候 RNN 的 time step 不会一样, 或者在 training 和 testing 的时候, `batch_size` 和`time_step` 也不一样, 这时, Tensorflow 就头疼了，但培养它哦人称可以hold住

---

在 GPU 训练可以大幅提升运算速度. Torch 有一套很好的 GPU 运算体系.但是：

* 你的电脑里有合适的 GPU 显卡(NVIDIA), 且支持 CUDA 模块.

* 必须安装 GPU 版的 Torch.
将每次的要计算的 data 变成 GPU 形式. + `.cuda()`

如果还有些计算是需要在 CPU 上进行的话, 比如 `plt` 的可视化, 我们需要将这些计算或者数据转移至 CPU.

`cpu_data = gpu_data.cpu()`

---

``

解决过拟合：

1。增加数据量, 大部分过拟合产生的原因是因为数据量太少了. 如果我们有成千上万的数据, 红线也会慢慢被拉直, 变得没那么扭曲 

2。运用正规化. L1, l2 regularization等等,

3。使用dropout函数，`torch.nn.Dropout(0.5) 随即选取一部分忽略掉`

---

批标准化, 和普通的数据标准化类似, 是将分散的数据统一的一种做法, 也是优化神经网络的一种方法，Normalization 的，具有统一规格的数据, 能让机器学习更容易学习到数据之中的规律.数据分布对训练会产生影响.  对隐藏层的输入结果进行像之前那样的normalization 处理

---

激励函数：非线性方程

优化器：optimizer： SGD ，momentu， Adam 等，为了加快训练效率。
