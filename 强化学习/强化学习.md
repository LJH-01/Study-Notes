

# 强化学习

### 概念

> 强化学习是一类算法，**让计算机实现从无到有的的学习过程**。具有**分数导向性**。
>
> 用**智能体Agent**来表示一个具备行为能力的物体，强化学习考虑的问题就是**智能体Agent**和**环境Environment**之间交互，包含了一系列的**动作Action**,**观察Observation**还有**反馈值Reward**。
>
> 动作集合的数量将直接影响整个任务的求解难度，DQN算法（不考虑其变种）仅适用于离散输出问题。状态State到动作Action的过程就称之为一个**策略Policy**强化学习的任务就是找到一个最优的策略Policy从而使Reward最多。

### MDP（Markov Decision Process）

简单的说就是下一个状态仅取决于当前的状态和当前的动作。注意这里的状态是完全可观察的全部的环境状态（也就是上帝视角）

### 算法分类

* 通过价值选行为：**Q Learning**、**Sarsa**、**Deep Q Network** 

* 直接选行为：**Policy Gradients**

* 想象环境并从中学习：**Model based RL**

### 方法汇总

1. 根据强化学习的方法是否理解所处的环境，分为model-free和model-based

	* Model-free RL 方法有很多，像Q learning,Sarsa,Policy Gradients都是从环境中得到反馈然后从中学习

	* Model-based RL 比前述多了一道程序，为真实世界建模，多了一个虚拟环境，在虚拟环境中还是Model-free方式。

2. 基于概率和基于价值

	* 基于概率是强化学习中最直接的一种，分析所处环境，输出下一步要采取动作的概率，根据概率采取行动。Policy Gardients对于选取连续的动作基于概率的方法有优势。

	* 基于价值会输出下一步所有动作的价值，根据最高的价值选择动作。Q leaning，Sarsa

	* 两种方法的结合 Actor-Critic Actor会基于概率做出动作，Critic会对做出的动作给出动作的价值。从而加速学习过程

3. 回合更新和单步更新
	* 回合结束更新
	* 每一步都更新。

4. 在线学习和离线学习

	* 在线学习：本人在场，边玩边学习。Sarsa，优化Sarsa lambda
	
	* 离线学习：可以选择自己玩也可以选择看着别人玩。也可以白天玩，晚上学。Q leaning Deep-Q-Network。

### 1.Q-Learning

![7caf29ae74d8dd72948060e56643b359.png](image/7caf29ae74d8dd72948060e56643b359.png)

![9e87bc490400e4356dd7a24fa656544c.png](D:/StudyNotes/强化学习/image/9e87bc490400e4356dd7a24fa656544c.png)

每次更新都用到了 Q 现实和 Q 估计, 而且在 Q(s1, a2) 现实 中, 也包含了一个 Q(s2) 的最大估计值, 将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实。

> ε-grerdy----------是用在决策上的一种策略, 比如 epsilon = 0.9 时, 就说明有90% 的情况我会按照 Q 表的最优值选择行为, 10% 的时间使用随机选行为。
>
> α--------------------是学习率, 来决定这次的误差有多少是要被学习的, alpha是一个小于1 的数. 
>
> γ--------------------是对未来 reward 的衰减值.

gamma = 1 时, 在 s1 看到的 Q 是未来没有任何衰变的奖励, 能清清楚楚地看到之后所有步的全部价值, 但是当 gamma =0, 只能摸到眼前的 reward, 同样也就只在乎最近的大奖励, 如果 gamma 从 0 变到 1, 对远处的价值看得越清楚, 所以机器人渐渐变得有远见。

**大多数RL是由reward导向的，所以定义reward是RL中比较重要的一点。**

------

### 2.Sarsa

sarsa 和Q-learning 的最大区别在于更新Q表的方法不同。

![38948267d6f13e848d541d6f21289ce5.png](image/38948267d6f13e848d541d6f21289ce5.png)

![f818ff626d82bf808bfa7f640347ae2b.png](image/f818ff626d82bf808bfa7f640347ae2b.png)

可以说Sarsa是一种比较保守胆小的算法，而Q-learning是一种贪婪的，大胆的算法。每走一步就更新一次自己的行为准则叫做srasa（0），每走多步才更新一次就叫做Sarsa（n）

![144b4b2b69d92fd26108988a9e5c02c9.png](image/144b4b2b69d92fd26108988a9e5c02c9.png)

![453b16d687d8e595bfdf9e23858138e2.png](image/453b16d687d8e595bfdf9e23858138e2.png)

Sarsa-lambda 是基于 Sarsa 方法的升级版, 他能更有效率地学习到怎么样获得好的 reward. 如果说 Sarsa 和 Qlearning 都是每次获取到 reward, 只更新获取到 reward 的前一步. 那 Sarsa-lambda 就是更新获取到 reward 的前 lambda 步. lambda 是在 [0, 1] 之间取值,

如果 lambda = 0, Sarsa-lambda 就是 Sarsa, 只更新获取到 reward 前经历的最后一步.如果 lambda = 1, Sarsa-lambda 更新的是获取到 reward 前所有经历的步.

--------

### 3.Deep Q Network

> Google Deep mind 

Deep Q Network 是一种融合了神经网络和Q Learning的方法。

---------

### 4.Policy Gradients

**Policy Gradients 直接输出动作的最大好处就是, 它能在一个连续区间内挑选动作**，是 RL 中另外一个大家族, 不像 Value-based 方法 (Q learning, Sarsa), 但也要接受环境信息 (observation), 不同的是他输出不是 action 的 value, 而是具体的那一个 action。这样 policy gradient 就跳过了 value 这个阶段。 

而且个人认为 Policy gradient 最大的一个优势是: 输出的这个 action 可以是一个连续的值, 

-----------------

### 5.Actor Critic

* Actor Critic 合并了以值为基础（Q learning）和以动作概率为基础（比如 Policy Gradients-梯度原则）两类强化学习算法。

* Actor 的前生是Policy Gradients, 这让它能在连续动作中选取合适的动作

* Critic 的前生是 Q-learning 或其他以值为基础的学习法 , 能进行单步更新。而 Policy Gradients 是回合更新, 降低了学习效率.

* Actor 基于概率选行为, Critic 基于 Actor 的行为评判行为的得分, Actor 根据 Critic 的评分修改选行为的概率.

------------------

#### 5.1Deep Deterministic Policy Gradient（DDPG）

>Google DeepMind

* 吸收Actor-Critic 让 Policy gradient单步更新的精华, 还吸收DQN的精华

* 一句话概括 DDPG: 使用 Actor Critic 结构, 但是输出的不是行为的概率, 而是具体的行为, 用于连续动作 (continuous action) 的预测. DDPG 结合了之前获得成功的 DQN 结构, 提高了 Actor Critic 的稳定性和收敛性.

------------------

#### 5.2Asynchronous Advantage Actor-Critic （A3C）

> Google DeepMind

* 一种有效利用计算资源, 能提升训练效用的算法, Asynchronous Advantage Actor-Critic, 简称 A3C.

* 一句话概括 A3C: 一种解决 Actor-Critic 不收敛问题的算法. 它会创建多个并行的环境, 让多个拥有副结构的 agent 同时在这些并行环境上更新主结构中的参数. 并行中的 agent 们互不干扰, 而主结构的参数更新受到副结构提交更新的不连续性干扰, 所以更新的相关性被降低, 收敛性提高.
* A3C 的算法实际上就是将 Actor-Critic 放在了多个线程中进行同步训练. 可以想象成几个人同时在玩一样的游戏, 而他们玩游戏的经验都会同步上传到一个中央大脑. 然后他们又从中央大脑中获取最新的玩游戏方法.

-----

#### 5.3Distributed Proximal Policy Optimization（ 分布式近端策略优化）（DPPO）

> OpenAI 

* 一句话概括 PPO: 一种解决 Policy Gradient 不好确定 Learning rate (或者 Step size) 的问题. 因为如果 step size 过大, 学出来的 Policy 会一直乱动, 不会收敛, 但如果 Step Size 太小, 对于完成训练, 我们会等到绝望. PPO 利用 New Policy 和 Old Policy 的比例, 限制了 New Policy 的更新幅度, 让 Policy Gradient 对稍微大点的 Step size 不那么敏感。

* PPO 是基于 Actor-Critic 算法。

