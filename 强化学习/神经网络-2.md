# 神经网络-2 高级神经网络结构

## 1.卷积神经网络 CNN

1. 卷积神经网络CNN 最常应用的方面是计算机图片识别。

> * 过滤器：它不断地在图片中收集小的像素块
> * 池化（pooling）：每一次卷积的时候，神经层可能会无意间丢失一些信息，池化操作解决这一问题，在卷积的时候不压缩图片的宽高，压缩的工作留给池化。而且池化是一个筛选过滤的过程, 能将 layer 中有用的信息筛选出来, 给下一个层分析
> * 全连接层（fully connect）： 流行的一种结构：  图片 ------ 卷积-------池化-------卷积------池化-----全连接-----全链接-----分类器

2. 搭建一个CNN

	* 一个卷积神经网络一般包括：

		``` python 
		class CNN(nn.Module):
		    def __init__(self):
		        super(CNN, self).__init__()
		        self.conv1 = nn.Sequential( # input shape (1, 28, 28)
		            nn.Conv2d(				# 一个Conv2d是一个过滤器
		                in_channels=1,      # input height
		                out_channels=16,    # n_filters
		                kernel_size=5,      # filter size
		                stride=1,           # filter movement/step
		                padding=2,      	# 如果想要 con2d 出来的图片长宽没有变化, padding=
		            ),    					# (kernel_size-1)/2 当 stride=1
		                    				# output shape (16, 28, 28)
		            nn.ReLU(),    			# activation
		            nn.MaxPool2d(kernel_size=2),    # 在 2x2 空间里向下采样, 
		        )    								# output shape (16, 14, 14)
		        self.conv2 = nn.Sequential(  		# input shape (16, 14, 14)
		            nn.Conv2d(16, 32, 5, 1, 2),  	# output shape (32, 14, 14)
		            nn.ReLU(),  					# activation
		            nn.MaxPool2d(2),  				# output shape (32, 7, 7)
		        )
		        self.out = nn.Linear(32 * 7 * 7, 10)   	# fully connected layer,
		        										# output 10 classes
		
		    def forward(self, x):
		        x = self.conv1(x)
		        x = self.conv2(x)
		        x = x.view(x.size(0), -1)   # 展平多维的卷积图成 (batch_size, 32 * 7 * 7)
		        output = self.out(x)
		        return output
		
		```

	* Optimizer 和  LossEntropy 是训练的时候才用的到

	* ```pyton
		loss = loss_func(output, b_y)   # cross entropy loss
		        optimizer.zero_grad()           # clear gradients for this training step
		        loss.backward()                 # backpropagation, compute gradients
		        optimizer.step()                # apply gradients
		```

## 递归/循环神经网络RNN

1. 简介：

	> * RNN神经网络具备记住之前发生的事的能力，在分析 Data0 的时候, 我们把分析结果存入记忆. 然后当分析 data1的时候, NN会产生新的记忆, 我们就简单的把老记忆调用过来, 一起分析. 如果继续分析更多的有序数据 , RNN就会把之前的记忆都累积起来, 一起分析。
	> * 数学描述： 每次 RNN 运算完之后都会产生一个对于当前状态的描述 , state. 用S( t) 代替, 然后RNN开始分析 x(t+1) , 根据 x(t+1)产生s(t+1), 不过此时 y(t+1) 是由 s(t) 和 s(t+1) 共同创造的。
	> * 递归神经网络RNN最常用在语言识别。RNN的结构比较自由。

2. LSTM-RNN 当下最流行的的RNN之一

	> * 因为有梯度消失和梯度爆炸问题，所以RNN通常无法会议久远的记忆，
	> * 梯度消失和梯度爆炸：在误差反向传递的时候，会乘上一个参数，如果大于1，累乘后梯度爆炸；如果小于1，累乘后梯度消失。
	> * LSTM 就是为了解决这个问题而诞生的. LSTM 和普通 RNN 相比, 多出了三个控制器. (输入控制, 输出控制, 忘记控制

3. 搭建RNN循环神经网络

```python 
# 用LSTM-RNN做的分类手写数据
class RNN(nn.Module):
    def __init__(self):
        super(RNN, self).__init__()

        self.rnn = nn.LSTM(     # LSTM 效果要比 nn.RNN() 好多了
            input_size=28,      # 图片每行的数据像素点
            hidden_size=64,     # rnn hidden unit 神经元个数
            num_layers=1,       # 有几层 RNN layers
            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)
        )

        self.out = nn.Linear(64, 10)    # 输出层

    def forward(self, x):
        # x shape (batch, time_step, input_size)
        # r_out shape (batch, time_step, output_size)
        # h_n shape (n_layers, batch, hidden_size)   LSTM 有两个 hidden states, h_n 是分线, h_c 是主线
        # h_c shape (n_layers, batch, hidden_size)
        r_out, (h_n, h_c) = self.rnn(x, None)   # None 表示 hidden state 会用全0的 state

        # 选取最后一个时间点的 r_out 输出
        # 这里 r_out[:, -1, :] 的值也是 h_n 的值
        out = self.out(r_out[:, -1, :])
        return out
```

```python 
RNNn
```

