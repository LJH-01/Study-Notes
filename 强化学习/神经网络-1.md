# 神经网络-1

## 1.神经网络基本概念

* **误差反向传递**：（为了优化权重矩阵中的权重）前向传递输入信号直至输出产生误差，反向传播误差信息更新权重矩阵。权重得以在信息双向流动中得到优化。对比预测结果和真实结果的差别，以这种差别反向的传递回去，对每个神经元向正确的方向上改动一点点
* **Input Layer **：输入层直接接受信息
* **Hidden Layer **: 隐藏层 可以有很多层，负责传入信息的加工处理
* **Output Layer**：输出层 
* **激活函数** ： 每个神经元都有属于他的激活函数 Activation Function ，对每一次输入可能只有部分神经元被激活。然后是反向传递，激活的神经元又会更容易被修改。如此下来，会使神经网络变得对“图片”里的关键信息更加敏感
* **梯度下降** ：Gradient Descent 神经网络的原理就是一种梯度下降的机制，Optimization问题大家族中的一种，cost function cost= （predicate - y) ^2 用来计算预测值和实际值的误差，对于这个二次函数，梯度即斜率，梯度最小的时候即斜率最小，即误差最小，我们让这个梯度不断降低，最终找到最优点。这个二次函数只是一个示例，原理如此，实际更复杂。实际上梯度下降到0的点不止一个，所以所得到的最优解是局部最优解而不是全局最优解。
* **神经网络黑盒**  input-layer  hidden-layer  output-layer  
* **激励函数** ：用来解决不能用线性方程所概括的问题 "`relu`" 、“`sigmoid`” 、“ `tanh`” 可以自己创建自己的激励函数，但要办证激励函数是可微分的，因为 误差反响传递的时候只有可微分的函数才能将误差传递回去。对于神经网络不是很多，只有两三层的时候可以使用任意的激活函数，但是当神经层比较多的时候随意选择会造成梯度爆炸或梯度消失
* **激励函数的选择** ：CNN中推荐 ```relu ```；在RNN中推荐` tanh / relu `
* **优化器**：optimizer 加速神经网络训练过程

## 2.搭建一个神经网络

### 关系拟合类型（回归）

```python 
class Net(torch.nn.Module):  # 继承 torch 的 Module
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()     # 继承 __init__ 功能
        # 定义每层用什么样的形式
        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # 隐藏层线性输出
        self.predict = torch.nn.Linear(n_hidden, n_output)   # 输出层线性输出

    def forward(self, x):   # 这同时也是 Module 中的 forward 功能
        # 正向传播输入值, 神经网络分析出输出值
        x = F.relu(self.hidden(x))      # 激励函数(隐藏层的线性值)
        x = self.predict(x)             # 输出值
        return x
    ------------------
    # optimizer 是训练的工具
optimizer = torch.optim.SGD(net.parameters(), lr=0.2)  # 传入 net 的所有参数, 学习率
loss_func = torch.nn.MSELoss()      # 预测值和真实值的误差计算公式 (均方差)

for t in range(100):
    prediction = net(x)     # 喂给 net 训练数据 x, 输出预测值

    loss = loss_func(prediction, y)     # 计算两者的误差

    optimizer.zero_grad()   # 清空上一步的残余更新参数值
    loss.backward()         # 误差反向传播, 计算参数更新值
    optimizer.step()        # 将参数更新值施加到 net 的 parameters 上
```

1. 定义net，继承自```torch.nn.moudle```
2. 在``` __init__ ``` 中定义各个神经层```torch.nn.linner```，每一层输入的数量，输出的数量。
3. 在forward中定义，神经层的传播关系。每一个隐藏层要过一个激励函数
4. 建立net实例。
5. 指定优化器和学习效率：```optimizer = torch.optim.SGD(net.parameters(), lr=0.2)  ```Stochastic gradient descent 
6. 指定误差函数 ```loss_func = torch.nn.MSELoss()MSELoss```损失函数中文名字就是：均方损失函数，公式如下所示：![img](https://img2018.cnblogs.com/blog/1003156/201810/1003156-20181025114456815-1725580994.png)
7. 训练开始
	1. 得到预测值，
	2. 计算预测值和真实值的误差
	3. 误差反向传递
	4. 反向传播优化，optimizer

### 分类 

1. 误差函数不同，对于分类问题（多分类） `CrossEntropyLoss() `

## 3.快速搭建

之前是写了一个net class，torch中可以有更加便捷的方式。

```torch.nn.sequential()```，在这个sequential中一层一层的累积即可，

```python
net2 = torch.nn.Sequential(
    torch.nn.Linear(1, 10), # 一个输入产生10个输出
    torch.nn.ReLU(), 		# 经过一个激活函数，这里的Relu大写，是一个class
    torch.nn.Linear(10, 1)	# 上层10个输入，产生一个输出。
)
```

## 4.保存提取

保存网络两中方式：

1. 保存整个神经网络
2. 只保存网络中的参数，速度快占用内存少

```python
torch.save(net1, 'net.pkl')  # 保存整个网络
torch.save(net1.state_dict(), 'net_params.pkl')   # 只保存网络中的参数 (速度快, 占内存少)
```

提取网络两种方式：

1. 方式一，提取整个网络当网络比较大的时候可能会有点慢

```pyton
def restore_net():
    # restore entire net1 to net2
    net2 = torch.load('net.pkl') # 
```

2. 方式二，只提取网络参数，需要新建一个和保存的网络结构一模一样的网络

```python
 # 新建 net3
    net3 = torch.nn.Sequential(
        torch.nn.Linear(1, 10),
        torch.nn.ReLU(),
        torch.nn.Linear(10, 1)
    )

    # 将保存的参数复制到 net3
    net3.load_state_dict(torch.load('net_params.pkl'))
```

## 5.批训练

> 批训练对于训练速度，训练效果等都有提升,mini-batch 训练。

```python 
# 先转换成 torch 能识别的 Dataset
torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)

# 把 dataset 放入 DataLoader
loader = Data.DataLoader(
    dataset=torch_dataset,      # torch TensorDataset format
    batch_size=BATCH_SIZE,      # mini batch size
    shuffle=True,               # 要不要打乱数据 (打乱比较好)
    num_workers=2,              # 多线程来读数据
)
```

## 6.加速神经网络训练(optimizer)

* `Stochastic Gradient Descent (SGD)`
* `Momentum`
* ``AdaGrad``
* `RMSProp`
* `Adam`

